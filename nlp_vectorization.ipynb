{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T23:42:18.933297Z",
     "start_time": "2022-01-31T23:42:08.263491Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Notice that these vectorizers are from `sklearn` and not `nltk`!\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- normalize a lexicon with stemming and lemmatization\n",
    "- run feature engineering algorithms for NLP\n",
    "    - bag-of-Words\n",
    "    - vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:21.505642Z",
     "start_time": "2022-02-01T00:06:20.552479Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-55e42f568536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m manual_cleanup = [s.translate(str.maketrans('', '', string.punctuation))\\\n\u001b[1;32m      5\u001b[0m                   for s in manual_cleanup]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m manual_cleanup = [token for token in manual_cleanup if\\\n\u001b[1;32m      8\u001b[0m                   token not in sw]\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n",
    "first_document = corpus.iloc[0].body\n",
    "manual_cleanup = [word.lower() for word in first_document.split(' ')]\n",
    "manual_cleanup = [s.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                  for s in manual_cleanup]\n",
    "sw = stopwords.words('english')\n",
    "manual_cleanup = [token for token in manual_cleanup if\\\n",
    "                  token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:34.458533Z",
     "start_time": "2022-02-01T00:06:34.359152Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c43d3d721687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c43d3d721687>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfirst_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_doc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sw' is not defined"
     ]
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[’'][a-z]+)?)\")\n",
    "first_doc = tokenizer.tokenize(first_document)\n",
    "first_doc = [token.lower() for token in first_doc]\n",
    "first_doc = [token for token in first_doc if token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:36.526353Z",
     "start_time": "2022-02-01T00:06:36.448502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noting',\n",
       " 'that',\n",
       " 'the',\n",
       " 'resignation',\n",
       " 'of',\n",
       " 'james',\n",
       " 'mattis',\n",
       " 'as',\n",
       " 'secretary',\n",
       " 'of',\n",
       " 'defense',\n",
       " 'marked',\n",
       " 'the',\n",
       " 'ouster',\n",
       " 'of',\n",
       " 'the',\n",
       " 'third',\n",
       " 'top',\n",
       " 'administration',\n",
       " 'official',\n",
       " 'in',\n",
       " 'less',\n",
       " 'than',\n",
       " 'three',\n",
       " 'weeks',\n",
       " 'a',\n",
       " 'worried',\n",
       " 'populace',\n",
       " 'told',\n",
       " 'reporters',\n",
       " 'friday',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unsure',\n",
       " 'how',\n",
       " 'many',\n",
       " 'former',\n",
       " 'trump',\n",
       " 'staffers',\n",
       " 'it',\n",
       " 'could',\n",
       " 'safely',\n",
       " 'reabsorb',\n",
       " 'jesus',\n",
       " 'we',\n",
       " 'can’t',\n",
       " 'just',\n",
       " 'take',\n",
       " 'back',\n",
       " 'these',\n",
       " 'assholes',\n",
       " 'all',\n",
       " 'at',\n",
       " 'once',\n",
       " 'we',\n",
       " 'need',\n",
       " 'time',\n",
       " 'to',\n",
       " 'process',\n",
       " 'one',\n",
       " 'before',\n",
       " 'we',\n",
       " 'get',\n",
       " 'the',\n",
       " 'next',\n",
       " 'said',\n",
       " 'year',\n",
       " 'old',\n",
       " 'gregory',\n",
       " 'birch',\n",
       " 'of',\n",
       " 'naperville',\n",
       " 'il',\n",
       " 'echoing',\n",
       " 'the',\n",
       " 'concerns',\n",
       " 'of',\n",
       " 'million',\n",
       " 'americans',\n",
       " 'in',\n",
       " 'also',\n",
       " 'noting',\n",
       " 'that',\n",
       " 'the',\n",
       " 'country',\n",
       " 'was',\n",
       " 'only',\n",
       " 'now',\n",
       " 'truly',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'reintegrate',\n",
       " 'former',\n",
       " 'national',\n",
       " 'security',\n",
       " 'advisor',\n",
       " 'michael',\n",
       " 'flynn',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'not',\n",
       " 'sustainable',\n",
       " 'i’d',\n",
       " 'say',\n",
       " 'we',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'maybe',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'more',\n",
       " 'former',\n",
       " 'members',\n",
       " 'of',\n",
       " 'trump’s',\n",
       " 'inner',\n",
       " 'circle',\n",
       " 'over',\n",
       " 'the',\n",
       " 'remainder',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " 'but',\n",
       " 'that’s',\n",
       " 'it',\n",
       " 'this',\n",
       " 'country',\n",
       " 'has',\n",
       " 'its',\n",
       " 'limits',\n",
       " 'the',\n",
       " 'u',\n",
       " 's',\n",
       " 'populace',\n",
       " 'confirmed',\n",
       " 'that',\n",
       " 'they',\n",
       " 'could',\n",
       " 'not',\n",
       " 'handle',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'shit',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'rejoin',\n",
       " 'society',\n",
       " 'at',\n",
       " 'once']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing\n",
    "\n",
    "#### Stemming\n",
    "Most of the semantic meaning of a word is held in the root, which is usually the beginning of a word.  Conjugations and plurality do not change the semantic meaning. \"eat\", \"eats\", and \"eating\" all have essentially the same meaning. The rest is grammatical variation for the sake of marking things like tense or person or number.   \n",
    "\n",
    "Stemmers consolidate similar words by chopping off the ends of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stemmer](img/stemmer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different stemmers available.  The two we will use here are the **Porter** and **Snowball** stemmers.  A main difference between the two is how aggressively it stems, Porter being less aggressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:46.172276Z",
     "start_time": "2022-02-01T00:06:46.102726Z"
    }
   },
   "outputs": [],
   "source": [
    "p_stemmer = nltk.stem.PorterStemmer()\n",
    "s_stemmer = nltk.stem.SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:48.089817Z",
     "start_time": "2022-02-01T00:06:47.988560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noting'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:49.310038Z",
     "start_time": "2022-02-01T00:06:49.232269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'note'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stemmer.stem(first_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:49.900042Z",
     "start_time": "2022-02-01T00:06:49.837550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'note'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_stemmer.stem(first_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:50.603861Z",
     "start_time": "2022-02-01T00:06:50.485956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was wa was\n",
      "jesus jesu jesus\n",
      "can’t can’t can't\n",
      "naperville napervil napervill\n",
      "was wa was\n",
      "this thi this\n",
      "i’d i’d i'd\n",
      "trump’s trump’ trump\n",
      "that’s that’ that\n",
      "this thi this\n",
      "has ha has\n"
     ]
    }
   ],
   "source": [
    "for word in first_doc:\n",
    "    p_word = p_stemmer.stem(word)\n",
    "    s_word = s_stemmer.stem(word)\n",
    "    \n",
    "    if p_word != s_word:\n",
    "        print(word, p_word, s_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:51.942339Z",
     "start_time": "2022-02-01T00:06:51.846957Z"
    }
   },
   "outputs": [],
   "source": [
    "first_doc = [p_stemmer.stem(word) for word in first_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:53.161821Z",
     "start_time": "2022-02-01T00:06:52.372763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAJuCAYAAACdTifhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABHWklEQVR4nO3dd3xcV5n/8e+jbllydxKnOsVxSEKaFEi3KcsuC0sNbSmhLN7CQiC7LLDs0svSlw4BfgFCWwgJYNMJsRPS5fSQ2Omku8SOZMuWLOn5/XHuSGNZlubO3Dt3NPfzfr30sjXynHmsMvrOOc89x9xdAAAASEZD1gUAAADUE8IVAABAgghXAAAACSJcAQAAJIhwBQAAkKCmrAsotmDBAl+8eHGqj7Fjxw7NmDEj8zGohVqohVqohVqopbZrmcratWs3ufvCPT7g7jXz1tXV5Wnr6empiTGSGoda0hsjqXGoJb0xkhqHWtIbI6lxqCW9MZIapx5rmYqkHp8gz7AsCAAAkCDCFQAAQIIIVwAAAAkiXAEAACSIcAUAAJAgwhUAAECCCFcAAAAJIlwBAAAkiHAFAACQIMIVAABAgghXAAAACSJcAQAAJIhwBQAAkCDCFQAAQIIIVwAAAAkiXAEAACSIcAUAAJAgwhUAAECCCFcAAAAJIlwBAAAkKNVwZWbnmtltZna7mb09zccCAACoBamFKzM7VtKbJT1N0vGSnm9mS9J6PAAAgFrQlOLYT5F0jbv3S5KZrZH0YkmfTPEx9+rJHbv0tI/+QT4yIrvk1xWNNaNJunDfJ/XUA2cnVB0AAKgX5u7pDGz2FEk/l3SqpB2SLpXU4+5vHffvVkhaIUmLFi3qWrlyZSr1bB8c0et+viGx8V57XKdetHRmRWP09/ervb098zGohVqohVqohVqoJb7u7u617t69xwfcPbU3SW+SdIOkyyV9TdLnJvv3XV1dnpaRkRHfMTjkV157ve8YHCr77dO/vdMPedcq/9Rv7qy4pp6enpoYI6lxqCW9MZIah1rSGyOpcaglvTGSGoda0hsjqXGSqmUqCpNGe+SZNJcF5e7fkvQtSTKzj0l6KM3Hm4yZqa25Ua2N4c9yzWlvkSRtGxhKqjQAAFBHUg1XZraPu28ws4MlvURhiXBa62wLn7LenbsyrgQAANSiVMOVpJ+a2XxJuyS9xd23pPx4qetsDZ+ybTuZuQIAAHtKe1nwzDTHz0JnW7MkqY9wBQAAJsAO7TF1RMuC9FwBAICJEK5iKvRc9dFzBQAAJkC4imm054qZKwAAMAHCVUyFnqteeq4AAMAECFcxtTU3qMGkwaERDQwNZ10OAACoMYSrmMxM7c0mie0YAADAnghXZWhvCp82+q4AAMB4hKsyFGau2OsKAACMR7gqwwzCFQAA2AvCVRnam8Onjb2uAADAeISrMow2tNNzBQAAxiFclYGeKwAAsDeEqzIUlgWZuQIAAOMRrsrQ3hRmrnrpuQIAAOMQrsowOnPFsiAAABiHcFUGtmIAAAB7Q7gqA1cLAgCAvSFclYF9rgAAwN4QrsrAVgwAAGBvCFdlmNFEuAIAABMjXJVhJvtcAQCAvSBclWFGUUO7u2dcDQAAqCWEqzI0NZjamhs0POLasWs463IAAEANIVyVqaO1WRJ9VwAAYHeEqzLNamuSRLgCAAC7I1yVqWM0XLHXFQAAGEO4KlNnFK64YhAAABQjXJWpo5VlQQAAsCfCVZk620JD+zbCFQAAKEK4KlNh5qqXnisAAFCEcFWmWfRcAQCACRCuytTBVgwAAGAChKsy0XMFAAAmQrgq0+jVggP0XAEAgDGEqzJ1siwIAAAmQLgqE+EKAABMhHBVptGeK64WBAAARQhXZRrboZ2eKwAAMIZwVabRswVZFgQAAEUIV2Wa2dIkM2n74LCGRzzrcgAAQI0gXJWpocHU0cIu7QAAYHeEqwqM7dJO3xUAAAgIVxXo5HxBAAAwDuGqAmNXDBKuAABAQLiqAOcLAgCA8QhXFSj0XPXScwUAACKEqwrMoucKAACMQ7iqAD1XAABgPMJVBei5AgAA4xGuKsD5ggAAYDzCVQUK+1z10XMFAAAihKsKjIYrlgUBAECEcFUBeq4AAMB4qYYrM3uHmd1uZreZ2Q/NrC3Nx6u20Z6rAXquAABAkFq4MrMDJL1NUre7HyupUdIr03q8LIyeLcjMFQAAiKS9LNgkaYaZNUlql/RIyo9XVR30XAEAgHHM3dMb3OxcSR+VtEPS79z91RP8mxWSVkjSokWLulauXJlaPZLU39+v9vb2RMYYGHb9/cWPq7lB+tFL98u0lkpRC7VQC7VQC7VkPUat1TKV7u7ute7evccH3D2VN0lzJf1R0kJJzZJ+Juk1k92nq6vL09bT05PYGCMjI374e37ph7xrle/cNZRpLbUwDrWkN0ZS41BLemMkNQ61pDdGUuNQS3pjJDVOUrVMRVKPT5Bn0lwWfLak+9x9o7vvknSxpNNSfLyqMzP6rgAAwG7SDFd/kXSKmbWbmUl6lqQ7Uny8TNB3BQAAiqUWrtz9WkkXSbpB0q3RY52f1uNlpbM12uuKXdoBAIDC1Xypcff3S3p/mo+RtcLMVS/nCwIAALFDe8Vm0XMFAACKEK4qNLpLO+EKAACIcFWx0fMF6bkCAAAiXFVs7GpBeq4AAADhqmKFfa76mLkCAAAiXFWsk54rAABQhHBVodGeK8IVAAAQ4apiY1cL0nMFAAAIVxUbPVuQnisAACDCVcU4WxAAABQjXFVoVtRzRbgCAAAS4api9FwBAIBihKsKdRT1XLl7xtUAAICsEa4q1NzYoLbmBo241D84nHU5AAAgY4SrBHC+IAAAKCBcJaCTvisAABAhXCWgk+0YAABAhHCVAPa6AgAABYSrBHS20nMFAAACwlUCxmau6LkCACDvCFcJoOcKAAAUEK4SMHa1IOEKAIC8I1wlgH2uAABAAeEqAfRcAQCAAsJVAjqLzhcEAAD5RrhKQAc9VwAAIEK4SkCh54pwBQAACFcJ6KTnCgAARAhXCaDnCgAAFBCuEkDPFQAAKCBcJWBmS5PMpP7BYQ2PeNblAACADBGuEtDQYOpoiZYGmb0CACDXCFcJGW1qH6CpHQCAPCNcJaSDw5sBAIAIV4nhfEEAACARrhIzdsUgy4IAAOQZ4SohnSwLAgAAEa4SQ7gCAAAS4Sox9FwBAACJcJUYeq4AAIBEuErM6PmCLAsCAJBrhKuEcL4gAACQCFeJKfRc9dFzBQBArhGuEjJ2tSA9VwAA5BnhKiGjPVfMXAEAkGuEq4TQcwUAACTCVWJG97kiXAEAkGuEq4SwQzsAAJAIV4lpbWpQc6NpcHhEA0PDWZcDAAAyQrhKiJnRdwUAAAhXSaLvCgAAEK4SxMwVAABILVyZ2VIzu6nordfM3p7W49WC0ab2ATYSBQAgr5rSGtjd10k6QZLMrFHSw5IuSevxagGHNwMAgGotCz5L0j3u/kCVHi8To+cLEq4AAMitaoWrV0r6YZUeKzOFniuOwAEAIL/M3dN9ALMWSY9IOsbdH5/g4yskrZCkRYsWda1cuTLVevr7+9Xe3p7KGN+/tU8X37ldrzq2Q2c/pSPTWrIYh1qohVqohVqopZ5qmUp3d/dad+/e4wPunuqbpBdK+l0p/7arq8vT1tPTk9oYX77sLj/kXav8Y7/6c+a1ZDEOtaQ3RlLjUEt6YyQ1DrWkN0ZS41BLemMkNU5StUxFUo9PkGeqsSz4KuVgSVCi5woAAKTcc2Vm7ZL+StLFaT5Orehs5WpBAADyLrWtGCTJ3fslzU/zMWrJ2OHN7HMFAEBesUN7grhaEAAAEK4SRM8VAAAgXCVobFmQcAUAQF4RrhJEzxUAACBcJWhmUc+Vp7w5KwAAqE2EqwQ1NzZoRnOjRlzqHxzOuhwAAJABwlXCOtq4YhAAgDwjXCWMvisAAPKNcJWwwi7tXDEIAEA+Ea4Sxl5XAADkG+EqYezSDgBAvhGuEkbPFQAA+Ua4SlgHu7QDAJBrhKuE0XMFAEC+Ea4S1knPFQAAuUa4Shg9VwAA5BvhKmHs0A4AQL4RrhJGzxUAAPlGuEpYBzu0AwCQa4SrhM2i5woAgFwjXCWMnisAAPKNcJUweq4AAMg3wlXC2psbZSb1Dw5reMSzLgcAAFQZ4SphDQ02dngzs1cAAOQO4SoFhV3a+wZoagcAIG8IVymg7woAgPwiXKWAKwYBAMgvwlUKOF8QAID8IlylgF3aAQDIL8JVCui5AgAgvwhXKeik5woAgNwiXKVgdCsGeq4AAMgdwlUKRq8WZFkQAIDcIVylgJ4rAADyi3CVgtGrBem5AgAgdwhXKZjFPlcAAOQW4SoF7NAOAEB+Ea5SQM8VAAD5RbhKQaHniqsFAQDIH8JVCsbOFiRcAQCQN4SrFLQ2Nai50TQ4PKKBoeGsywEAAFVEuEqBmdF3BQBAThGuUkLfFQAA+US4Sgl9VwAA5BPhKiVju7SzkSgAAHlCuEoJPVcAAOQT4SolhWVBeq4AAMgXwlVKOjlfEACAXCJcpWT0akHOFwQAIFcIVymh5woAgHwiXKWko7AsyMwVAAC5QrhKySz2uQIAIJcIVykZ26GdhnYAAPIk1XBlZnPM7CIzu9PM7jCzU9N8vFpCzxUAAPnUlPL4n5f0G3c/28xaJLWn/Hg1g6sFAQDIp9TClZnNknSWpNdLkrsPShpM6/FqDWcLAgCQT+bu6QxsdoKk8yX9WdLxktZKOtfdt4/7dyskrZCkRYsWda1cuTKVegr6+/vV3l7ZBFopY/QNjuj1P9+gmc2m775o30xrqdY41EIt1EIt1EIt9VTLVLq7u9e6e/ceH3D3VN4kdUsakvT06P3PS/rwZPfp6urytPX09FRljF1Dw37Iu1b5oe9e5SMjI5nWUq1xqCW9MZIah1rSGyOpcaglvTGSGoda0hsjqXGSqmUqknp8gjyTZkP7Q5Iecvdro/cvknRSio9XU5oaGzSjuVEjLvUPDmddDgAAqJLUwpW7PybpQTNbGt30LIUlwtyg7woAgPxJ+2rBt0r6fnSl4L2S3pDy49WUjrYmbegb0LaBXZLasi4HAABUQarhyt1vUui9yqXCXle9zFwBAJAb7NCeos7RXdoJVwAA5AXhKkX0XAEAkD+EqxSN7dLO+YIAAOQF4SpFnC8IAED+EK5S1MGyIAAAuUO4StEswhUAALlDuEoRPVcAAOQP4SpF9FwBAJA/hKsUFXqutg0QrgAAyAvCVYoK+1yxQzsAAPlBuErR2A7t9FwBAJAXhKsU0XMFAED+EK5SRM8VAAD5Q7hK0cyWRplJ/YPDGhoeybocAABQBYSrFJnZ6F5X2weGM64GAABUA+EqZbOivqtemtoBAMgFwlXKxnZpp+8KAIA8IFylrJPzBQEAyBXCVcrGrhhkWRAAgDwgXKWMva4AAMgXwlXKCj1XhCsAAPKBcJWyWfRcAQCQK4SrlI1dLUjPFQAAeUC4ShlXCwIAkC+Eq5R1RA3t2whXAADkAuEqZYWZq17CFQAAuUC4SlknPVcAAOQK4Spl7HMFAEC+EK5SNrZDO+EKAIA8IFyljKsFAQDIF8JVykb3uSJcAQCQC4SrlLU1N6qlsUGDwyPauWs463IAAEDKCFdVQN8VAAD5QbiqAvquAADID8JVFdB3BQBAfhCuqmBs5oqNRAEAqHeEqyroaI02EqXnCgCAuke4qoJZ9FwBAJAbhKsqGL1akGVBAADqHuGqCrhaEACA/CBcVUGh54p9rgAAqH+xw5WZzTWz49Iopl4VZq56mbkCAKDulRSuzGy1mc0ys3mSbpZ0gZl9Nt3S6kcnO7QDAJAbpc5czXb3XkkvkXSBu3dJenZ6ZdUX9rkCACA/Sg1XTWa2SNLLJa1KsZ66NNpzxbIgAAB1r9Rw9UFJv5V0t7tfb2aHSborvbLqC1cLAgCQH00l/rtH3X20id3d76XnqnSjZwvScwUAQN0rdebqiyXehgnMagvLgr30XAEAUPcmnbkys1MlnSZpoZmdV/ShWZIa0yysnsxsDZ+qbQNDcneZWcYVAQCAtEw1c9UiqUMhhHUWvfVKOjvd0upHU2OD2lsa5S5tHxzOuhwAAJCiSWeu3H2NpDVm9m13f6BKNdWljtYm9Q8Oa9vOodEeLAAAUH9K/S3fambnS1pcfB93f2YaRdWjzrYmbegbUN/OXdpvdlvW5QAAgJSUGq5+Iulrkr4piXWtMnRETe19XDEIAEBdKzVcDbn7V+MObmb3S+pTCGRD7t4dd4x6MYu9rgAAyIVSw9VKM/sXSZdIGijc6O5PlHDfZ7j7pnKKqyeje10RrgAAqGulhqtzoj/fWXSbSzos2XLqF+cLAgCQD+bu6Q1udp+kLQpB7Ovufv4E/2aFpBWStGjRoq6VK1emVo8k9ff3q729vepjXHBTr1bd1a9zju/UC46cmWktaY1DLdRCLdRCLdRST7VMpbu7e+2ELU/uPuWbpNdN9FbC/faP/txH0s2Szprs33d1dXnaenp6Mhnjs79b54e8a5V/5nfrMq8lrXGoJb0xkhqHWtIbI6lxqCW9MZIah1rSGyOpcZKqZSqSenyCPFPqsuDJRX9vk/QsSTdI+u5kd3L3R6I/N5jZJZKeJunyEh+zrhSWBem5AgCgvpUUrtz9rcXvm9lsSRdOdh8zmympwd37or8/R9KHyi10uqPnCgCAfCh3q/B+SUum+Df7SrokOkevSdIP3P03ZT7etNfRGva52sY+VwAA1LWSwpWZrVRoSpfCgc1PkfTjye7j7vdKOr6i6upIJ/tcAQCQC6XOXH266O9Dkh5w94dSqKdudRTCFTNXAADUtYZS/pGHA5zvlNQpaa6kwTSLqkez6LkCACAXSgpXZvZySddJepmkl0u61szOTrOwejPac8WyIAAAda3UZcH3SjrZ3TdIkpktlPQHSRelVVi9oecKAIB8KGnmSmFLhQ1F72+OcV9Iam9pVINJO3YNa2h4JOtyAABASkqdufqNmf1W0g+j918h6VfplFSfzEwdrU3q3TmkbQNDmtPeknVJAAAgBZOGKzM7QtK+7v5OM3uJpDMkmaSrJX2/CvXVlc62ZvXuHFLfTsIVAAD1aqqlvf+V1CdJ7n6xu5/n7u9QmLX633RLqz/0XQEAUP+mCleL3f2W8Te6e4+kxalUVMc6WqPzBdnrCgCAujVVuGqb5GMzkiwkDzhfEACA+jdVuLrezN48/kYze5OktemUVL862jhfEACAejfV1YJvVzh8+dUaC1PdklokvTjFuupSYeaql54rAADq1qThyt0fl3SamT1D0rHRzb909z+mXlkd6iz0XBGuAACoWyXtc+Xul0m6LOVa6h49VwAA1D92Wa8irhYEAKD+Ea6qqDNqaGefKwAA6hfhqoo62EQUAIC6R7iqInquAACof4SrKupsZZ8rAADqHeGqijhbEACA+ke4qqJCzxUzVwAA1C/CVRUV91y5e8bVAACANBCuqqi1qVEtjQ3aNewaGBrJuhwAAJACwlWV0XcFAEB9I1xVGX1XAADUN8JVlbHXFQAA9Y1wVWWj5wuyLAgAQF0iXFVZ4XzBXsIVAAB1iXBVZZ2t9FwBAFDPCFdVRs8VAAD1jXBVZaNXC7IsCABAXSJcVVmh56qPZUEAAOoS4arKClcLsokoAAD1iXBVZfRcAQBQ3whXVdbJDu0AANQ1wlWVjfZcsSwIAEBdIlxVGTu0AwBQ3whXVUbPFQAA9Y1wVWWdrWzFAABAPSNcVVlHUUP7iHvG1QAAgKQRrqqsscHU3tIod2lgiHAFAEC9IVxloNB31b+LcAUAQL0hXGWgcMVgPzNXAADUHcJVBgp7XfXvGsm4EgAAkDTCVQZYFgQAoH4RrjJQCFc7CFcAANQdwlUGRnuuWBYEAKDuEK4yMNpzRUM7AAB1h3CVAWauAACoX4SrDNBzBQBA/SJcZYCrBQEAqF+EqwywzxUAAPWLcJWBsZ4rZq4AAKg3qYcrM2s0sxvNbFXajzVdjPZccbUgAAB1p6kKj3GupDskzarCY00LhXDVNzCix3t3VjTWEzuGKx5DkkacoAcAQBJSDVdmdqCk50n6qKTz0nys6aTQc/XY9mE9/WOXVj7gqsrHOGZhi37ZXXkpAADknXmKMxZmdpGkj0vqlPTv7v78Cf7NCkkrJGnRokVdK1euTK0eServ71d7e3umY7i7PnnVVq3fPCgzq6gWd694jCcHRjTi0lf/dqH2mdlY9jhJfG6TGodaqIVaqIVaqCVt3d3da919z6kJd0/lTdLzJX0l+vtySaumuk9XV5enraenpybGSGqcJMZY8d3r/ZB3rfLvXXN/5rUkNQ61pDdGUuNQS3pjJDUOtaQ3RlLjUEt6Y5RCUo9PkGfSbGg/XdILzOx+ST+S9Ewz+16Kj4cyLTtyH0nSmnUbM64EAIDpL7Vw5e7vcfcD3X2xpFdK+qO7vyatx0P5li1dKEm68u5NGhxi7y0AACrBPlfQAXNm6MBZTdo+OKy1D2zJuhwAAKa1qoQrd1/tEzSzo3actF+LJGn1+g0ZVwIAwPTGzBUkSSfs1yqJvisAACpFuIIk6egFLZrR3Kg7H+vTY09WvikpAAB5RbiCJKm50XTa4fMlSZevZ/YKAIByEa4wanl01SB9VwAAlI9whVGF/a6uuGuThobZkgEAgHIQrjDq4PntOmzBTPXtHNKND27NuhwAAKYlwhV2c9aRYWmQqwYBACgP4Qq7KfRdraGpHQCAshCusJtTDpuv1qYG3frwk9rYN5B1OQAATDuEK+ymrblRpxwWtmS44i5mrwAAiItwhT0si/quVtN3BQBAbIQr7KHQd3XFXRs1POIZVwMAwPRCuMIeDl0wUwfNm6Et/bt0y0Nbsy4HAIBphXCFPZiZlkcbinLVIAAA8RCuMCH6rgAAKA/hChM69fD5amls0M0PbdWW7YNZlwMAwLRBuMKEZrY26eRD58pdupwtGQAAKBnhCntF3xUAAPERrrBXy6ItGS5fv1EjbMkAAEBJCFfYqyX7dGj/2W3atG1Qf360N+tyAACYFghX2CszG529Wr1uQ8bVAAAwPRCuMKll9F0BABAL4QqTOv2I+WpqMN3wl616cseurMsBAKDmEa4wqc62ZnUdMlfDI64r796UdTkAANQ8whWmRN8VAAClI1xhSsX7XbmzJQMAAJMhXGFKT1nUqX06W/V474DufKwv63IAAKhphCtMycxGD3LmqkEAACZHuEJJ6LsCAKA0hCuU5MwjFqrBpJ77t2jbwFDW5QAAULMIVyjJ7PZmnXjwXA2xJQMAAJMiXKFk9F0BADA1whVKtjzqu1qzji0ZAADYG8IVSnbs/rM1f2aLHt66Q/ds3JZ1OQAA1CTCFUrW0GA668jCVYMsDQIAMBHCFWKh7woAgMkRrhDLmUsWyEy69t4n1D/IlgwAAIxHuEIs8ztaddwBszU4PKJr7t2cdTkAANQcwhViW7Y0HORM3xUAAHsiXCE2+q4AANg7whViO+GgOZo9o1kPbO7X/Zu2Z10OAAA1hXCF2BobTGcuWSCJg5wBABiPcIWyLI/6rlgaBABgd4QrlOWsI8PM1dX3btbOXcMZVwMAQO0gXKEs+3S26Zj9Z2nnrhFdd98TWZcDAEDNIFyhbMs4CgcAgD0QrlC2sb4rmtoBACggXKFsJx48R52tTbpn43Y9+ER/1uUAAFATCFcoW3Njg86ItmTgqkEAAALCFSpC3xUAALsjXKEiy5aGcHXVPZu0a8QzrgYAgOwRrlCRRbNnaOm+neofHNadmwazLgcAgMwRrlCx5dHs1Y2PEa4AAEgtXJlZm5ldZ2Y3m9ntZvbBtB4L2Sr0Xd346EDGlQAAkL2mFMcekPRMd99mZs2S/mRmv3b3a1J8TGSge/E8tbc06i+9Q/pJz4Oa2VrZt1VjP8fpAACmr9TClbu7pG3Ru83RGx3PdailqUGnHb5Af7jjcb3zolsqHm9RR6Oec4bLzBKoDgCA6rKQgVIa3KxR0lpJR0j6sru/a4J/s0LSCklatGhR18qVK1OrR5L6+/vV3t6e+Rj1Vsv9W3fpotuflFtjRXXc8vig+odcn3vOfB08u7nscWrl80It1EIt1EIt07eWqXR3d6919+49PuDuqb9JmiPpMknHTvbvurq6PG09PT01MUZS49RbLe/40Y1+yLtW+flr7sm8lqTGoZb0xkhqHGpJb4ykxqGW9MZIapx6rGUqknp8gjxTlasF3X2rpNWS/qYaj4fpq7Bv1mrOKwQATFNpXi240MzmRH+fIenZku5M6/FQH85cslAm6fr7tmj7wFDW5QAAEFuaM1eLJF1mZrdIul7S7919VYqPhzowb2aLjpjXrMHhEV19z+asywEAILY0rxa8RdKJaY2P+nXifi2664ldWrN+o5599L5ZlwMAQCzs0I6ac+J+rZJC35WneDUrAABpIFyh5hw+r1lz25v14BM7dN+m7VmXAwBALIQr1JxGM525JLpqcN3GjKsBACAewhVqUuG8wjXrCVcAgOmFcIWadFYUrq65d7N27uKsQQDA9EG4Qk1a2NmqYw+YpYGhEV1zL1syAACmD8IVatbyI/eRxNIgAGB6IVyhZhWOwllDUzsAYBohXKFmnXjQHHW2NeneTdv1l839WZcDAEBJCFeoWU2NDTpzyQJJ0hoOcgYATBOEK9Q0+q4AANMN4Qo1rbAlw1X3bNbAEFsyAABqH+EKNW2/2W06ar9O9Q8Oq+f+LVmXAwDAlAhXqHmFqwZXr6PvCgBQ+whXqHn0XQEAphPCFWpe1yFzNbOlUesf36ZHtu7IuhwAACZFuELNa2lq0OlHFLZkYPYKAFDbCFeYFui7AgBMF4QrTAvLoi0Zrrx7s3YNj2RcDQAAe0e4wrRw4Nx2HbFPh7YNDGntA2zJAACoXYQrTBvLo9kr+q4AALWMcIVpY6zvinAFAKhdhCtMGycvnqcZzY2649FePd67M+tyAACYEOEK00Zbc6NOPXy+JJYGAQC1i3CFaWUZfVcAgBpHuMK0sjzqu7pi/UYNsSUDAKAGEa4wrRwyf6YWz29X784h3fzQ1qzLAQBgD4QrTDvLl4aDnLlqEABQiwhXmHbouwIA1DLCFaadUw6br5amBt3y0JPatG0g63IAANgN4QrTzoyWRj390HmSpCvuYvYKAFBbCFeYlui7AgDUKsIVpqVC39Xl6zdqeMQzrgYAgDGEK0xLhy+cqQPnztCW/l269eEnsy4HAIBRhCtMS2Y2dtUgS4MAgBpCuMK0Ndp3tX5DxpUAADCGcIVp69TD56u50XTzg1u1Zftg1uUAACCJcIVprKO1SScvnqcRl664e1PW5QAAIIlwhWmOvisAQK0hXGFaK/RdrVm/USNsyQAAqAGEK0xrR+7bof1mtWnTtgH9+dHerMsBAIBwhenNzLR8KQc5AwBqB+EK0x59VwCAWkK4wrR3+pIFamwwrf3LFj25Y1fW5QAAco5whWlvVluzug6eq+ER11VsyQAAyBjhCnVhWdR3tZqlQQBAxghXqAujfVfrN8qdLRkAANkhXKEuHLP/LC3sbNVjvTu17vG+rMsBAOQY4Qp1wcx01hKuGgQAZI9whbqxnL4rAEANIFyhbpy5ZIEaTOp54Ant2DWSdTkAgJwiXKFuzGlv0QkHzdGuYdetGwazLgcAkFOphSszO8jMLjOzO8zsdjM7N63HAgqWHRkOcr7xsYGMKwEA5FWaM1dDkv7N3Z8i6RRJbzGzo1N8PGC07+rGxwbYkgEAkImmtAZ290clPRr9vc/M7pB0gKQ/p/WYwFMPmK15M1u0cfug3vuz29TW1FjReBs29OqXj1T2LZvEGNRCLdRS+RgNJh3ZtktdFY0CTM2q8erezBZLulzSse7eO+5jKyStkKRFixZ1rVy5MtVa+vv71d7envkY1JLeOF+8bqtWP7Cz4joA1J+DOhv0v3+zT0Vj1MpzHbWkX8tUuru717p79/jbUw9XZtYhaY2kj7r7xZP92+7ubu/p6Um1nrVr16qrq7LXLUmMQS3pjbN524C++strtd/+B1Zcy0MPPaQDD6xsnCTGoBZqoZbKx/js79erf3BY17znWdpvdlvZ49TKcx21pF/LVMxswnCV2rJg9KDNkn4q6ftTBSsgKfM7WvXcI2aqq+uwisdau3ZLxeMkMQa1UAu1VD7GNfc+oT/c8bjWrN+gV5x8cEVjAZNJ82pBk/QtSXe4+2fTehwAAEpROOB9zXo2Gka60rxa8HRJr5X0TDO7KXr72xQfDwCAvVoeHfB+xV2bNDTMRsNIT5pXC/5JkqU1PgAAcRw0r137dzbqkb4h3fjgVp28eF7WJaFOsUM7ACA3TtqvVZK0et2GjCtBPSNcAQBy44QoXNF3hTQRrgAAuXHMwha1NTfotod7taGP/fCQDsIVACA3WhpNpxw2X5J0xfpNGVeDekW4AgDkSuGqwdUsDSIlhCsAQK4sWxqOv7niro0aHuGAdySPcAUAyJVDF8zUIfPbtbV/l25+aGvW5aAOEa4AALmzLFoaXLOOpUEkj3AFAMid5Uvpu0J6CFcAgNw55bD5amls0C0PbdUT2wezLgd1hnAFAMid9pYmPf2weXIPje1AkghXAIBcou8KaSFcAQByqdB3tWb9Ro2wJQMSRLgCAOTS4Qs7dMCcGdq8fVC3P9KbdTmoI4QrAEAumZmWFa4aXLch42pQTwhXAIDcGu27YksGJIhwBQDIrdOPWKCmBtMNf9miJ/t3ZV0O6gThCgCQWx2tTepePFcjLv3p7k1Zl4M6QbgCAOTa8uggZ/qukBTCFQAg14r7rtzZkgGVI1wBAHLtqP06te+sVm3oG9Adj/ZlXQ7qAOEKAJBrZsZVg0gU4QoAkHv0XSFJhCsAQO6dfsQCNTaY1j6wRX072ZIBlSFcAQByb/aMZp108BwNjbiuvHtz1uVgmiNcAQAgdmtHcghXAABorO9qzboNbMmAihCuAACQdPSiWVrQ0aJHntypuzdsy7ocTGOEKwAAJDU0mM6KlgZXr2NpEOUjXAEAEKHvCkkgXAEAEDlzyUKZSdfd94S2DwxlXQ6mKcIVAACReTNbdPyBczQ4PKJr7mVLBpSHcAUAQJFl9F2hQoQrAACKLF8ahav1bMmA8hCuAAAoctyBczS3vVkPPrFD923annU5mIYIVwAAFGlsMJ25hKsGUT7CFQAA49B3hUoQrgAAGKewmeg1927Wzl3DGVeD6YZwBQDAOAs7W3XsAbM0MMSWDIiPcAUAwASWHxkd5EzfFWIiXAEAMIFl0ZYMa+i7QkyEKwAAJnDiQXPU2dakezdt118292ddDqYRwhUAABNoamzQmUsWSJLWrN+QcTWYTghXAADsBX1XKAfhCgCAvShsyXDVPZs1MMSWDCgN4QoAgL3Yb3abjtqvU/2Dw+q5f0vW5WCaIFwBADCJwlWDq9fRd4XSEK4AAJgEfVeIi3AFAMAkug6Zq5ktjVr/+DY9snVH1uVgGiBcAQAwiZamBp1+RGFLBmavMDXCFQAAU6DvCnEQrgAAmMKyaEuGK+/erKERz7ga1DrCFQAAUzhwbruO2KdD2waGtG7zrqzLQY1LLVyZ2f8zsw1mdltajwEAQLUsj2avbnxsIONKUOvSnLn6tqS/SXF8AACqptB3deOjhCtMrimtgd39cjNbnNb4AABU08mL52lGc6Puf3JIL/zSnySzisbr375d7ddcmfkY9VrLlw/s05H7dlZcTznMPb3GvChcrXL3Yyf5NyskrZCkRYsWda1cuTK1eiSpv79f7e3tmY9BLdRCLdRCLdOvls9es1VXPriz4jqQvv951jwtmdeS6mN0d3evdffu8bdnHq6KdXd3e09PT2r1SNLatWvV1dWV+RjUQi3UQi3UMv1qGRga1s8uu15HLl1acS133nmnjjrqqMzHqNdaXrDsZM1sTW2BTpJkZhOGq3QfFQCAOtLa1Kgj5jXrxIPnVjzWyMaWisdJYox6rSXtYDUZtmIAAABIUJpbMfxQ0tWSlprZQ2b2prQeCwAAoFakebXgq9IaGwAAoFaxLAgAAJAgwhUAAECCCFcAAAAJIlwBAAAkiHAFAACQIMIVAABAgghXAAAACSJcAQAAJIhwBQAAkCDCFQAAQIIIVwAAAAkiXAEAACSIcAUAAJAgwhUAAECCCFcAAAAJIlwBAAAkiHAFAACQIMIVAABAgghXAAAACTJ3z7qGUWa2UdIDKT/MAkmbamAMaqEWaqEWaqEWaqntWqZyiLsv3ONWd8/Vm6SeWhiDWqiFWqiFWqilFsagluTfWBYEAABIEOEKAAAgQXkMV+fXyBhJjUMt6Y2R1DjUkt4YSY1DLemNkdQ41JLeGEmNU4+1lKWmGtoBAACmuzzOXAEAAKSGcAUAAJAgwhWAKZnZoaXcBgAgXE3JzC6M/jw3gbFaS7ltujCzRjN7R9Z1oCp+OsFtF1W9CkzIzBrM7LSs60havT1nSpKZzTCzpVnXUY/MbPEEt52cQSlqyuJBq8nM9pX0MUn7u/tzzexoSae6+7dKHKLLzA6R9EYz+64kK/6guz8Ro5yrJZ1Uwm2TMrNz3f3zU902xRjtkv5N0sHu/mYzWyJpqbuvKnUMdx82sxdK+lyp95mknn0lFX4IrnP3DWWOUcnXunis0yQtVtHPiLt/N8b9Pybpk+6+NXp/rqR/c/f/ilnHsZKOltQWtw4zm/T7yt1vKGGMoyQdI2m2mb2k6EOzimuKw8xmKHzfrSvn/kkxswvd/bVT3TbFGIdKetTdd0bvz5C0r7vfH7OWIyV9NbrvsWZ2nKQXuPtHSrm/u4+Y2WcknRrnccfVcJS737m375tSv18qHWOcip4zk/gZSJKZ/Z2kT0tqkXSomZ0g6UPu/oIS7/8ad/+emZ030cfd/bMxajlX0gWS+iR9U9KJkt7t7r8rdYykmNlMSTui7+MjJR0l6dfuvivmUBeb2d+5+8PRuMskfUnSU5OteGp1H64kfVvhG+i90fvrJf2fpFJ/4X5N0m8kHSZpbdHtJsmj2ydlZvtJOkDSDDM7UWMBbZak9hLrKHaOpPFB6vUT3DaZCxT+P4Un44ck/URSyeEqcqWZfUnhc7q9cGOcJy0ze7mkT0larfC5+aKZvdPd486MfFuVfa0L9Vwo6XBJN0kajm52SSWHK0nPdff/LLzj7lvM7G8llRyuzOz9kpYrhKtfSXqupD/FqOMz0Z9tkrol3azw+T1O0rWSzihhjKWSni9pjqS/K7q9T9KbS6xjVCW/XMysT+HrsMeHJLm7z4pZzjHjxm+S1BVzjJ9IKp4xGo5ui/tq+RuS3inp65Lk7reY2Q8klRSuIr8zs5dKutjLuwz8PEkrNPZ9U8wlPbNKYyT5nDlRHbHriWp6iaRPSNonqqec77sPSHqawnOd3P2miWZbJjEz+rNzgo/F/Zq/0d0/b2Z/LWmhpDcoPH+WFK7M7E/ufsYEP5flfF4ul3Rm9CL0Ukk9kl4h6dUxxpCkf5T0s+h55iSFF9t/G3OMZGS5PXw13iRdH/15Y9FtN5UxzlclHS/prdHb8THue46kyxR+IV1W9PYLSS+JMc6rJK2UtCW6b+HtMkl/iPn/6Zng83JzGZ+XyyZ4+2PMMW6WtE/R+wvLrCWpr/UdirYpqeD77hZJrUXvz5B0e8wxblVYur85en9fSSvLqOVHkp5a9P6xkr4dc4xTK/l8FI2zVtLscV+jW5IYO0YN74l+Fock9Ra9bZb08Zhj7fH9ldX3bvR/GpE0GP1/+iT1xhyjQdLp1fx67KWORJ4zE67pbklPqXCMayf4Osf+/p/oaxT361Z4XIUX5S8eX1eVP7c3RH++VdJ/VFKLwoTBLZKuk7Qwi/+Pu+di5mq7mc1XlKzN7BRJT5Yxzp2SvifpYoVkfqGZfcPdvzjVHd39O5K+Y2YvdfeJeldKdZWkRxUOpCx+Rdan8M0Ux2C0hFH4vBwuaSBuQe7+jLj3mUCD774MuFnl9QMm9bW+TdJ+Cp/rcn1P0qVmdkFUzxslfSfmGDs9TJMPmdksSRtUwkzpBI5y91sL77j7bdGMURwvNrPbJe1QmMk9XtLb3f17MccZcvcnzWzqfzmOmc2b7ONe4hK9u39c0sfN7OOSPinpSI0tccZ99b/RzF7g7r+IanyhyjssdlP0M1j43j1bMb//3H2i2YxYou+3T6uC5cWCSpbWk3rOHLeUPdHjXBxjuMfd/Y5ya4ncZmZ/L6kxasV4m8Lzelxf1J5LoxPdNpm1ZvY7SYdKeo+ZdSqE85KZWYNCSDs2zv0mHspOVZipelN0W8n5xMxWavef3XaF5/5vmZm8xGXXJOUhXJ2n8GrncDO7UmFW5OwyxnmTpFPcfbskmdknFNb+pwxXBe7+UzN7nsJyRHEPzYdKvP8Dkh5QAk98kt6v8EvyIDP7vqTTFZYWY0moz+nXZvZbST+M3n+FwjJYXBN9rV9WxjgLJP3ZzK5TUeCM8wPq7p80s1slPUshjH/Y3X8bs47rzWyOwpLRWknbFF6NxXWHmX1TIfC5pNcozM7F8Rx3/w8ze7HCEvLLFGYS4oarSn65rFWovziZFd4vaYl+nHsVliMOVFgCPkXhZ7rkpSJJ/yTp+9HSuEl6UNLrYtYhSW9R2FH6KDN7WNJ9Cl+nWKJllSXa/fnl8pjDVLq8mNTSuhReoHxW0lnR+2sUlpFLfdFUWMreR2H59o/R+89QWJqLE656zOz/JP1Muz8vxBnjrQptCwMKz3e/lfThUu8cBZDTJC0c13c1S1JjjDqk8DvtBEnNCm0DCxRaK0oWhfGbzexgd/9LzMcvdq7CjPIl7n67mR2m8PxSqk9X8NipyMUO7VEvxVKFJ791Hr9JTtEvypN9rHG1TWEqv+RGOTP7mkKifoZCA+HZCs3bb5r0jmP3T2yNO3ryu1VhJuJehenq2K+4zezXivqc3P346HN9Y8zPyyc01gNkCr/wTnH3d8WspVXhiXz0a60wKxZrRi5qgtyDu6+JM06loq/R5ZKukLRT0ix3jztDWfhe/WeN/YK6XNJXC9/LJY5xu7sfY2bfkPRTd/+Nmd3s7sfHrKVd4ZfLcxS+Rr9VCJ4l1xKNM097hohYX5/Cz7Ska9z9BAvN+x9091fEGScaq0Ph+bQv7n3HjTNT4Xs29jhm9g8Kv6R2C4vuHicsFnrbZiosm+5Uec8vd0g6utxwVjTOTxVmkguzvq9VaMmYdEZqgnFWSXqzuz8avb9I0pfjjBPNQo/n7v7GOLVUInpuWq4Q6r9W9KE+hZaBu2KMldT3yx8Vfo6u0+59t1WdLTKzRkm/dfdnV/Nx9yYv4aqiK7+iMc5T6AO4JLrpRQp9K/8bY4xb3P24oj87FF4dPidOLUkws2cqhJkzFV7x3yTpco9xxWE0zvXufrKZ3ejuJ0a33eTuJ8QY4wZ3P2ncbbe4+3Exa5lonD1uq4ZoSfKLkp6i0LzdKGl7zF9QiXyNkmBm/6PwPb9DoSF3jqRV7v70CsZslDTT3Xtj3m+iXwpXufuzYo5T+N69SdLT3X2gjO/dVkkv1Z7PLyXNRic5TpJhsVJm9hNJbyuEmQrG2ePrEfdrFN3ntuKlqwSXs2Ixs25J/6k9v85xn+v+w90/Oe62l7n7T2KMkcj3SxIvRi1cIfjv2vPzEjfo/ULSa2PMbKam7pcFk5qedvfPmtlqjc2uvMHdb4xZzo7oz34z21+htyiTjRjd/Y9mtkbhh+sZCq+EjlG8Kw6lCvqczOyfJf2LpMPMrHhGplPSlaUWYAldWZTkzKDC5b+vVLhyrFthqeiIGPev+GsUPXnu9dVTnCd0d393NMPY62ELju2SXljq/Ytq+oHC/2NYUXO7mX3W3T8VY5hzNfZL4RmFXwpxa5H0ULTs+jNJvzezLZIeiTnGzxW+39eqjJ7FhMfZ6e47zUxm1uphS4TY+ymZ2aXjg+pEt+3lvoXel05VuLQe2WFmZ7j7n6LxT9fY82gcq4taD1zhZzPOspPM7AsT3PykwsVBPy9xmO8rXBV6q2L2N43zSoV+wWLvUXi+KVUi3y8Jzej/RGEm7psa+z1djp2SbjWz32v3WbS3VVZefHUfrhR+sVU8PS2Nbi9Qyb4oq6In809F47jCN1PVmdmlClP/VyssO53sZewtpcp62n4g6deSPi7p3UW393m8/cP+WqFf7EBJxfu89Cm8SiyJu58R/VlxY3A0zt1m1ujuw5IuMLNYjasJfI2eH+fxpqjldUV/L/5Q3B6ao92918xerdBX9y6FQBEnXCX1S+HF0V8/YGaXKVzF+JuYwxzo7n8T97FTGqeisBgtH7dLWmChd6v4Rcr+JQ7z6eh+n1CY6RwdProtrn9WaGyfHb2/RWEFIRZ3/1cLze1nRjed7+6XTHafCbQp7L9UCDAvlXS7pDeZ2TPc/e0ljLHRo4sfymFmz1XYWuCAcWFvlsIybhxJvLhIZJZe4UKXr8Z97An8MnrLXN0vCyY1PZ20aBmgLavpSzP7nMKePgMKs0SXK6y3x35VaAn0tCXBKr8aMzFmdrmkZyuE58cUrvx6fZwepSS/RpUys+ILN9oUGvVvcPdYF4dYuOLwBIVg/SV3X2Mxe7fM7BKFPXnertB8vkVSs7tXfT8bMztf0he96GrMLMcpGm+ZorDo7oMl3udchc/p/pIe1li46pX0DXf/UozHT2qpv1XhxdrhCkvRTyrMIsdadk1C1Fv0HHcfit5vUtgT6q8k3eruR5cwxrMUttS5VGU0xZvZ8Qo/Px+S9L6iD/VJuszdt5T0n9lz3NjfL0X37dGes/RLvGifvxLG+IDC1dCXaPfPS5wX2TWlbsPVuOnpExSa7SqZnk6qror7vxKup0PhF9W/S9rP3WMdLRH1zTxPe/6fSt4pOCnRq7D3qfwri5Ks5RCFJ4tmSe9QeOL6irvfXcZYlX6Nipc5W6Ka4r6yHD/mbEkXxv05MrO3KcxW3azwfXOwpO+5+5mT3nHv45X9SyEJZvZnhcb6exWeXwpLyHFDRFLjnKHwi+0CM1soqcPd74s5xlu9hC1m9nLf0aV+SfcUfahT0pXuHusKSDP7jaStCjP9o8tF7j7Z5qATjVPxBqBmtk7S0wrPJ9HPwLXufpQV9ZxOMcb3FGa/btfYsqB7zKZ4M2vO6kXseGbW4+7dxeHZzK5y95KPYzKzib5H3d1jXQFs4Qrkj2vPUy3K2cKmIvW8LJj09HTFkur/SqiWf1WYIu9S2N7h/yksPcW1UtE6tyrrIUjCtxSuLHp59P5rFa5kjHVlURI8bJshhf6QcvqBEvsajV/mNLMXKTSlV6JfIQzEreULkoqXMx4ws7L3Skuo36MSz5U0V2PLTZcrhIGqj2NhR/9uhVnkCxRC9PcUtlkpmbt/sYIXgUkt9Rcktez6SUl/55XtU/VJSTdFvbem8CLuYxau8PxDiWMc7zGupJ7EYgv7tGUeIhR6iFsUPjefVJilnznFfXbj7kn1Hl+gsM3Q5xT6VN8g7bZ1S9XUbbgqPOlGCX+3J2ALm2dmIbH+rwTMUOhPWluY5i7TgXFfXafocHd/adH7H7RwJVjVJNlEruS+RuNr+JmZvXvqfznGdt+kr1HhSf3HcR/b9rIvmmIeUVRDXiTpH1S0ubDCvmRxZ36SGOfFCufD3SBJ7v6IhY0hY6nkRWA0q/OkwtJXEq4ys6cmsFxa8Qag7v4tM/uVwgsTk/Sf7l7oUXpnicNcY2ZHu/ufK6lFNRQiFF7ENkj6V4VZ+oMU8wWthS1azlM4c3SFlXHWbWSGu19qZha9wP2AmV2h8LmqqnpeFkx0ejqhmmqy/6sSFq4gu9QzOOxzglqulvRO3/3Kok+7exKbrpZawyGTfbxoRqtqbPddqhsUQv6yOJ+XaPmt8GQxJOkBjw5HjVlLxfui1RILV7me6mObC89U6IuLu5xX8Thmdp27P63Q71RBLYnsUZWEaLn0CIVNVStZLv28wqkLP1PMXidL8DDq6HN7uCr//6x19y4zu7Xws2NmV5S7vF4JMzvXx20PM9FtU4zxfwoXtrzOw8HlMxS+d0+IWcuVCrO/FylsGPuwpP9x99gXvFSqbmeulPz0dNks+cuTa8k1ki6xsG/MLpW3bUFSErmyqBJZhKcSFB+4PCTpfpW4jYJF21MoHOhdvDu6m5lLekLSp9z9KyXWssDdf2xm75Ekdx8ys0ouvc6aafdLx4dV3gxCEuP82My+LmmOmb1Z4cilb5RRSxLHPyXluQmNM0thKbt4T0FXaTu0J3IYdSSJJU5J2hk9594VtQ88rNBPloVztOf2MK+f4LbJHO7urzCzV0mSu+8wK+OMrHBBRrvCyQ8fVpjVK+fEhIrVbbhKYXq6EjXX/5Wgzygs69xaA69071Doiyi+suhFin/uYsXSaCIvl7u/oYL7Tro9hYU9zq6SVGq4Sur8x1pxgaRroysYpfD9Vs4SZxLjDCj0/vQq9F29z91/X0YtFR//lJSkXqxU+DOwIvqz7N7AorEekCQz20dFvVJleLsyDhFREPp7SYda2LyzYJbCHo5xJHLWbXT/CyUdovCcK4UXGFVvXanbcFVLarT/Kyl3SbqtBoKVFDZi3KrQcxJ7ySpJKTWRl8XMDlTo3Tld4cnnT5LOdfeHKh3b3Teb2fIYd0nqrM+a4MlsLpzUOPsqbLB6g8LFD6U2WY/3gTLvV7Ms7OH1Ju15rmvJV+mZ2csUrkrtM7P/Ujgk+cNxvk5m9gKFF6T7K1xNfIjCi8JjSh0jUgsh4iqF2c0F2n1Wr0/xX9AmctatktuktWJ123NVS2qx/yspZvZthf/Xr7X7q9wstmLY7YiLWmNm17j7KRk87u8VlskvjG56jaRXu/tfVbuWqJ6a2BetHkVLKc9RaHDuVrjo4Fvufs+kd6xzUb/rnQozLR+S9GpJd7j7uTHGKBxbdoZCu8mnFZraSz4CysxuVlhG/IO7nxhdKfuqwuxYjHHWaYIQkVVbQtRrusTd/xBNGDR5Cedjmtnp7n6lhf3MOhSOsjKF0xfKOeu20MaQOWauqqNm+r9ScF/01hK9ZSmpK4sqtpcm8qxeySx09+JDZ79tZm/PqBYpzOAtVnj+OcnMMt3rrZ64u5vZYwob1w4pbO9wkZn93t3/o5QxamlJO0FHuPvLzOyF7v4dC8cw/TbmGIWeuOcpHHz+cwubX8axK5rtbTCzBne/LLooKK6KdnpPUtTft0LSPIWWjAMVjrIp5azPLyhsNXO1h01nK91d/f1m9k2VuUlrkghXVVBj/V+JsbCB6JIamnk7Q9LrLWxIV/aVOAkpu4k8BZvM7DUK56pJ4fswbk9EIiq5zB+Ts7BB6zmSNimcDPBOd99VaHyWVFK4qqUl7QQVZke3mtmxCuFzccwxHo4uGHi2pE9Esy0NMcfYamFT4Cskfd/MNij+sTVSDYUISW9R+P64NqrhrqinrBS7zOwC7Xmcj6Kx4p4J+AaFTVqbVbRJq0q7cCFRhCuUzcMBvgvNrMUz2B17AkldWVSxShpoU/BGhYOkP6fwRHOVwpNQFmppr7d6s0DSS8YvDbn7iJmVfc6kl7EvWg0638J5if+l0PPXIem/Y47xcoWr/T7t7lvNbJFK39+q4AUKmy6fq7A8P0vlbTJcMyFC0oC7DxYu7ouW/Uv9+X6+Qlh9psJWDJU6vla2dSFcoVL3S7oyulqk+BTyqvdc1dI2CGZ2mMKlyKcoPNFcLekd7n5vBuV8WNI5Hp07ZmbzFPpFYh25kZBausy/rrj7+yb5WMkbaNbYknZSLlQ4aHmxpO9Et+0bZwB3749mms5QmAkciv6cUlEv0OMa+1wWthr4iJnF3dKkZkKEpDVm9p+SZpjZXyn0F68s5Y5RX9WPzOwOd785gVqS2qS1YoQrVOqR6K1BoUEfwQ8kfVlh12wpHGz6Q0klN78m6DgvOtDV3Z8wsynPQUtSne/1Vm9qaUk7KT9XaM1Yq/Iu8a/oeKEUtjSpmRChcFboPyg01/+jpF8pLEvHscPMLpW0b7SJ6HGSXuDuH4k5zhmSzqmF1hCuFkQiLByz4e6+LetaaoGZXTv+KqIMrxa8WdLycTNXa6r5ytfCDu+Ffd2Ke39M0ifiXHEFxJXElcQWjtI6UdINHh3SbEWHFVfKzBZ5iad3WEI7vVcq6ue7JYHP7RqFJdavF31uY3/NbC8nZGSxqsHMFSoSNYdeqHCliMxsk8IRBrdnWlj2Lov6VH6kMGPzCkm/jIKNqnyV6GcUrqS8KKrl5ZI+WsXHr/e93upKmvuiZSiJK4kHo6sxCxtdxjqceCqlBqtIUju9VyTq57vZzA52979UMFS7u19nu2/KHrvRv5ZaQwhXqNT5ks5z98skKdpQ8huSTsuwplrwiujPfxx3+xsVfmFV7fR6d/+umfUoNI2aQtNzVZcTivd6s3COXkGnpCurWQumdIHCsvbLovdfE92Wyb5olbCxg9SbJL3BzO5VGTM90f5hqyyZ44UqVkshQtIiSbdHS/3Ffbdxlvo3WdiVvRBcz9Y078tkWRAVMbOb3f34qW5Dvlk473Gu6nOvt7piZjf5uANzJ7ptOtjbMlFBnJBiZjco9Bc9RyGc/dbLO16orkRL/nsYP0M9xRiHKbxQP03hTNj7FDY6rqUQGQszV6jUvWb239p99+/7MqynJphZs8JB0mdFN61W6CfI5W7k9brXW52qmX3RKpXwL+erJW1197jbL9S7v0h61N13SqPL/CVdiWlm5xW9+ytJlylcHLVd4erOql91npS4G6ABkkY3g5TCZngLFfZXuURhr51a2uMpK19V2Hn4K9FbV3QbUOveqNCX95jC0szZ4mdaCocjX21m95jZLYW3rIuqAT/R7uf4DUe3laIzeutWeDE6V9IcSf8k6ejkSqw+Zq5Qrq5oyv0chScd0577t+TZyeOWRv8YXbUH1Lpa2hetltTMJsU1pql4E+loQ9GSjkJz9w9Kkpn9TtJJhfMIo2OFSg1oNYlwhXJ9TeEU88Mk9RTdXghZVWvYrlHDZna4RwfmRj0Fw1PcB6gFme+LVoumc/9Pyjaa2QsKZx2a2QsVjmCK42BJxad8DCr+8UQ1hXCFsrj7FyR9wcy+6u7/nHU9NeidCtsxFHZkXyyWVjA9NJjZ3HEzV/yuwN78k8I5iV9WeGH9kKTXxRzjQknXmdkl0Rgv1thO+tMSVwsCKTCzNkn/prGT4X8v6XOFpk+gVpnZ6yS9R9Ju+6K5+4WT3hG5Fh1IbYWlvTLuf5KkM6N3L3f3GxMrLgOEKyAFZvZjSb2Svh/d9CpJc939ZXu/F1AbzOxoje2LdmmNHLOCGmRm+0r6mKT93f250ffOqe7+rYxLyxThCkgB+38ByAMz+7XCJrPvdffjzaxJ0o01dLB0JtiKAUjHjWY2eo6gmT1d7EQOoP4scPcfK9qOwd2HxMU7NCkCKXm6pNeZWeG8rYMl3VE4jiOLU9oBIAXbzWy+xo6uOUVhw+BcI1wB6aiJg1UBIGXnSfqFpMPN7EqFTaXPzrak7BGugBSwJw6AnDhcYYPVgxSOrHm6yBb0XAEAgLL9t7v3Khxd82yFA5hzf9QX4QoAAJSr0Lz+PElfc/efSyrp+Jt6RrgCAADletjMvq6w2eyvzKxVZAv2uQIAAOUxs3aFC3hudfe7zGyRpKe6++8yLi1ThCsAAIAE5X7qDgAAIEmEKwAAgAQRrgDUHDN7r5ndbma3mNlN0fFBaT3WajPrTmt8APmT+42+ANQWMztV0vMlneTuA2a2QFzaDWAaYeYKQK1ZJGmTuw9IkrtvcvdHzOx9Zna9md1mZuebmUmjM0+fM7PLzewOMzvZzC42s7vM7CPRv1lsZnea2Xei2bCLoqucdmNmzzGzq83sBjP7iZl1RLf/j5n9Obrvp6v4uQAwDRGuANSa30k6yMzWm9lXzGxZdPuX3P1kdz9W0gyF2a2CQXc/S9LXJP1c0lskHSvp9dGhspK0VNL50aHZvZL+pfhBoxmy/5L0bHc/SVKPpPPMbJ6kF0s6JrrvR1L4PwOoI4QrADXF3bdJ6pK0QtJGSf9nZq+X9Awzu9bMbpX0TEnHFN3tF9Gft0q63d0fjWa+7lU480ySHnT3K6O/f0/SGeMe+hRJR0u60sxuknSOpEMUgthOSd80s5dI6k/q/wqgPtFzBaDmuPuwpNWSVkdh6h8lHSep290fNLMPSGorustA9OdI0d8L7xee58Zv6jf+fZP0e3d/1fh6zOxpkp4l6ZWS/lUh3AHAhJi5AlBTzGypmS0puukESeuiv2+K+qDOLmPog6NmeUl6laQ/jfv4NZJON7MjojrazezI6PFmu/uvJL09qgcA9oqZKwC1pkPSF81sjqQhSXcrLBFuVVj2u1/S9WWMe4ekc6Jz0O6S9NXiD7r7xmj58YfR+WhS6MHqk/RzM2tTmN16RxmPDSBHOP4GQN0zs8WSVkXN8ACQKpYFAQAAEsTMFQAAQIKYuQIAAEgQ4QoAACBBhCsAAIAEEa4AAAASRLgCAABI0P8HO9POb/f6D90AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fdist = FreqDist(first_doc)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:06:53.257468Z",
     "start_time": "2022-02-01T00:06:53.172984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming did not reduce our token count: 111 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'Stemming did not reduce our token count: {len(set(first_doc))} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing\n",
    "\n",
    "Lemmatizing is a bit more sophisticated than the stem choppers. Lemmatizing uses part-of-speech tagging to determine how to transform a word.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words, properly ensuring that the root word belongs to the language. It can handle words such as \"mouse\", whose plural \"mice\" the stemmers would not lump together with the original. \n",
    "\n",
    "- In Lemmatization, the root word is called the \"lemma\". \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "![lemmer](img/lemmer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:07:00.693995Z",
     "start_time": "2022-02-01T00:07:00.276580Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:07:01.345068Z",
     "start_time": "2022-02-01T00:07:01.246967Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f03d8faf2f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"Mice\" becomes: {lemmatizer.lemmatize(\"mice\")}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"Noting\" becomes: {lemmatizer.lemmatize(first_doc[0])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(f'\"Mice\" becomes: {lemmatizer.lemmatize(\"mice\")}')\n",
    "print(f'\"Noting\" becomes: {lemmatizer.lemmatize(first_doc[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T00:07:02.083174Z",
     "start_time": "2022-02-01T00:07:01.974182Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b38f5e181fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"He saw the trees get sawed down\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlemmed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlemmed_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b38f5e181fb4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"He saw the trees get sawed down\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlemmed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlemmed_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/Rob/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/share/nltk_data'\n    - '/opt/anaconda3/envs/learn-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# However, look at the output below:\n",
    "    \n",
    "sentence = \"He saw the trees get sawed down\"\n",
    "lemmed_sentence = [lemmatizer.lemmatize(token) for token in sentence.split(' ')]\n",
    "lemmed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers depend, for their full functionality, on POS tagging, and **the default tag is 'noun'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of work, we can POS tag our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[’'][a-z]+)?)\")\n",
    "first_doc = tokenizer.tokenize(first_document)\n",
    "first_doc = [token.lower() for token in first_doc]\n",
    "first_doc = [token for token in first_doc if token not in sw]\n",
    "corpus.loc[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's pos_tag to tag our words\n",
    "# Does a pretty good job, but does make some mistakes\n",
    "\n",
    "first_doc_tagged = pos_tag(first_doc)\n",
    "first_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then transform the tags into the tags of our lemmatizers\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc_tagged = [(token[0], get_wordnet_pos(token[1]))\n",
    "             for token in first_doc_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc_lemmed = [lemmatizer.lemmatize(token[0], token[1]) for token in first_doc_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc_lemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(set(first_doc_lemmed))} unique lemmas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(first_doc_lemmed)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Feature Engineering for NLP\n",
    "\n",
    "The machine learning algorithms we have encountered so far represent features as the variables that take on different value for each observation. For example, we represent individuals with distinct education levels, incomes, and such. However, in NLP, features are represented in a very different way. In order to pass text data to machine learning algorithms and perform classification, we need to represent the features in a sensible way. One such method is called **Bag-of-words (BoW)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words **because any information about the order or structure of words in the document is discarded**. The model is only concerned with whether known words occur in the document, not with **where** they may occur in the document. The intuition behind BoW is that a document is similar to another if they have similar contents. The Bag of Words method can be represented as a **Document Term Matrix**, in which each column is a unique vocabulary n-gram and each observation is a document. Consider, for example, the following **corpus** of documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document 1: \"I love dogs.\"\n",
    "- Document 2: \"I love cats.\"\n",
    "- Document 3: \"I love all animals.\"\n",
    "- Document 4: \"I hate dogs.\"\n",
    "\n",
    "This corpus can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\downarrow$Doc\\|Word$\\rightarrow$|I|love|dogs|cats|all|animals|hate\n",
    "-|-|-|-|-|-|-|-\n",
    "Document_1|1|1|1|0|0|0|0\n",
    "Document_2|1|1|0|1|0|0|0\n",
    "Document_3|1|1|0|0|1|1|0\n",
    "Document_4|1|0|1|0|0|0|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In order to get these tokens from our documents, we're going to use tools called \"vectorizers\".\n",
    "\n",
    "The most straightforward vectorizer in `sklearn.feature_extraction.text` is the `CountVectorizer`, which will simply count the number of each word type in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing it in python\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform([\" \".join(first_doc_lemmed)])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very exciting for one document. The idea is to make a document term matrix for all of the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body[0:2])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "X = vec.fit_transform(corpus.body[0:2])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document term matrix gets bigger and bigger, with more and more zeros, becoming sparser and sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set upper and lower limits to the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                      stop_words=sw, ngram_range=[1,2],\n",
    "                      min_df=2, max_df=25)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TfidfVectorizer`\n",
    "\n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common uses the TF-IDF algorithm -- \"Term Frequency-Inverse Document Frequency\". Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "- TF (Term Frequency)\n",
    "Term frequency is the frequency of the word in the document divided by the total words in the document.\n",
    "\n",
    "- IDF (inverse document frequency)\n",
    "Inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is generally calculated as the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$idf(w) = log (\\frac{number\\ of\\ documents}{num\\ of\\ documents\\ containing\\ w})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = tf_vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[313].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the tfidf to the count vectorizer output for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf lessoned the importance of some of the more common words, including a word, \"also\", which might have made it into the stopword list.\n",
    "\n",
    "It also assigns \"nerds\" more weight than power.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"Nerds\" only shows up in document 313: {len(df_cv[df.nerds!=0])} document.')\n",
    "print(f'\"Power\" shows up in {len(df_cv[df.power!=0])} documents!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words are stored in a `.vocabulary_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `HashingVectorizer`\n",
    "\n",
    "There is also a hashing vectorizer, which will encrypt all the words of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec = HashingVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                         stop_words=sw)\n",
    "X = hvec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules of thumb about these vectorizers:\n",
    "\n",
    "**Tf-Idf**: Probably the most commonly used. Useful when the goal is to distinguish the **content** of documents from others in the corpus.\n",
    "\n",
    "**Count**: Useful when the words themselves matter. If the goal is instead about identifying authors by their words, then the fact that some word appears in many documents of the corpus may be important.\n",
    "\n",
    "**Hashing**: The advantage here is speed and low memory usage. The disadvantage is that you lose the identities of the words being tokenized. Useful for very large datasets where the ultimate model may be a bit of a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For a final exercise, work through the following:\n",
    "\n",
    "Create a document term matrix of the 1000-document corpus. The vocabulary should have no stopwords, numbers, or punctuation, and it should be lemmatized. Use a `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
